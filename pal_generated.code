<GENERATED-CODE>
1. Update `wrangler.toml` to define a new Queue binding and a KV namespace binding.
2. Create `src/api_worker.ts` for the main API endpoint logic.
3. Create `src/queue_consumer_worker.ts` for the background processing logic.
4. Create `src/status_worker.ts` for polling job status.
5. Update `src/index.ts` (if it exists) or use the new files as entry points. I will assume `src/index.ts` is the main dispatcher and will provide updates to it.

<UPDATED_EXISTING_FILE: wrangler.toml>
# Example wrangler.toml. Adjust your existing file to include these bindings.

name = "your-project-name"
main = "src/index.ts" # or src/api_worker.ts if using separate entrypoints
compatibility_date = "2024-01-01"

[vars]
# Define any environment variables here, e.g., API keys
ISBNDB_API_KEY = "YOUR_ISBNDB_API_KEY"
GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"

[[queues.producers]]
binding = "HARVEST_BACKFILL_QUEUE" # Name for the queue binding in your Worker
queue = "harvest-backfill-queue"   # The actual name of your Cloudflare Queue

[[kv_namespaces]]
binding = "JOB_STATUS_KV"        # Name for the KV binding in your Worker
id = "YOUR_KV_NAMESPACE_ID"      # Replace with your actual KV Namespace ID

# If you're using D1 or R2, ensure those bindings are also present
# [[d1_databases]]
# binding = "DB"
# database_id = "YOUR_D1_DATABASE_ID"
# database_name = "your-database-name"
</UPDATED_EXISTING_FILE>

<NEWFILE: src/types.ts>
/**
 * src/types.ts
 *
 * Defines common types used across the API Worker, Queue Consumer, and Status Worker.
 */

export type JobStatus = 'QUEUED' | 'PROCESSING' | 'COMPLETED' | 'FAILED';

export interface BackfillJobPayload {
  userId: string;
  startDate?: string;
  endDate?: string;
  // Add any other data needed for the backfill operation
  [key: string]: any;
}

export interface BackfillJobMetadata {
  jobId: string;
  status: JobStatus;
  createdAt: string;
  startedAt?: string;
  completedAt?: string;
  progress?: string; // e.g., "Step 1/5: Generating books..."
  error?: string;
  results?: {
    totalRecordsProcessed?: number;
    newRecordsAdded?: number;
    // Add other relevant summary data
  };
}
</NEWFILE>

<NEWFILE: src/api_worker.ts>
import { Request, Router, IRequest } from 'itty-router';
import { Env } from './env'; // Assume an env.ts for environment types
import { BackfillJobPayload, JobStatus, BackfillJobMetadata } from './types';

// Define your router for the API endpoint
const apiRouter = Router({ base: '/api/harvest' });

/**
 * Handles the HTTP POST request to trigger a backfill job.
 * Validates the request, sends a message to the Cloudflare Queue,
 * and returns an immediate 202 Accepted response with a job ID.
 *
 * @param request The incoming HTTP request.
 * @param env The environment variables and bindings.
 * @param ctx The Worker context.
 * @returns A JSON response indicating the job was queued.
 */
apiRouter.post('/backfill', async (request: IRequest, env: Env, ctx: ExecutionContext) => {
  try {
    // 1. Validate request payload
    const payload: BackfillJobPayload = await request.json();
    if (!payload || !payload.userId) {
      return new Response(JSON.stringify({ error: 'Missing or invalid payload' }), {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    // 2. Generate a unique job_id
    const jobId = crypto.randomUUID();
    const createdAt = new Date().toISOString();

    // 3. Prepare job metadata for KV
    const jobMetadata: BackfillJobMetadata = {
      jobId,
      status: 'QUEUED',
      createdAt,
    };

    // Store initial job status in KV
    await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));

    // 4. Send job message to Cloudflare Queue
    // The queue message should contain the jobId and the full payload needed for processing.
    const queueMessage = {
      jobId,
      payload,
    };
    await env.HARVEST_BACKFILL_QUEUE.send(queueMessage);

    // 5. Return immediately with job_id
    return new Response(
      JSON.stringify({
        status: 'Job queued successfully',
        jobId,
        pollUrl: `/api/harvest/backfill/status/${jobId}`,
      }),
      {
        status: 202, // 202 Accepted indicates the request has been accepted for processing, but the processing has not been completed.
        headers: { 'Content-Type': 'application/json' },
      }
    );
  } catch (error) {
    console.error('Error in API Worker:', error);
    return new Response(JSON.stringify({ error: 'Internal server error' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
});

export default apiRouter;
</NEWFILE>

<NEWFILE: src/queue_consumer_worker.ts>
import { Env } from './env'; // Assume an env.ts for environment types
import { BackfillJobPayload, JobStatus, BackfillJobMetadata } from './types';

/**
 * Simulates calling the Gemini API.
 * @param query The input for Gemini.
 * @returns A simulated result.
 */
async function callGeminiAPI(query: string): Promise<string> {
  console.log('Calling Gemini API...');
  await new Promise(resolve => setTimeout(resolve, 10000)); // Simulate 10s delay
  return `Gemini result for "${query}"`;
}

/**
 * Simulates calling the ISBNdb API.
 * @param isbn The ISBN to look up.
 * @param env The environment.
 * @returns A simulated book title.
 */
async function callISBNdbAPI(isbn: string, env: Env): Promise<string> {
  // Respecting the 350ms delay, and the 3 req/sec rate limit.
  // In a real scenario, you'd want a proper rate-limiting mechanism,
  // potentially an external one or a custom queue for ISBNdb calls if scaling up.
  console.log(`Calling ISBNdb for ${isbn}...`);
  await new Promise(resolve => setTimeout(resolve, 350)); // Simulate 350ms delay
  // Add actual fetch logic here
  // const response = await fetch(`https://api2.isbndb.com/book/${isbn}`, {
  //   headers: { 'Authorization': env.ISBNDB_API_KEY },
  // });
  // const data = await response.json();
  // return data.book.title;
  return `Book Title for ${isbn}`;
}

/**
 * Simulates a database query for deduplication.
 * @param data The data to deduplicate.
 */
async function deduplicateDatabase(data: any): Promise<void> {
  console.log('Running database deduplication queries...');
  await new Promise(resolve => setTimeout(resolve, 5000)); // Simulate 5s delay
}

/**
 * Simulates ISBNdb batch enrichment.
 * @param isbns The ISBNs for enrichment.
 */
async function isbnDbBatchEnrichment(isbns: string[]): Promise<any> {
  console.log('Running ISBNdb batch enrichment...');
  const duration = Math.floor(Math.random() * (60 - 30 + 1)) + 30; // 30-60s
  await new Promise(resolve => setTimeout(resolve, duration * 1000));
  return { enrichedCount: isbns.length, ...isbns.map(isbn => ({ isbn, enrichedData: '...' })) };
}

/**
 * Simulates database writes.
 * @param data The data to write.
 */
async function writeToDatabase(data: any): Promise<void> {
  console.log('Writing to database...');
  await new Promise(resolve => setTimeout(resolve, 10000)); // Simulate 10s delay
}

export default {
  /**
   * Cloudflare Queue consumer entry point.
   * Processes a batch of messages from the queue.
   *
   * @param batch The batch of messages from the queue.
   * @param env The environment variables and bindings.
   * @param ctx The Worker context.
   */
  async queue(batch: MessageBatch<any>, env: Env, ctx: ExecutionContext): Promise<void> {
    for (const message of batch.messages) {
      const { jobId, payload } = message.body as { jobId: string; payload: BackfillJobPayload };

      let jobMetadata: BackfillJobMetadata | null = null;
      try {
        const existingStatus = await env.JOB_STATUS_KV.get(jobId);
        if (existingStatus) {
          jobMetadata = JSON.parse(existingStatus);
        } else {
          // This should ideally not happen if the API worker sets it immediately
          jobMetadata = { jobId, status: 'QUEUED', createdAt: new Date().toISOString() };
        }

        if (jobMetadata!.status === 'PROCESSING' || jobMetadata!.status === 'COMPLETED') {
          console.warn(`Job ${jobId} already processing or completed. Skipping.`);
          message.ack(); // Acknowledge to remove from queue
          continue; // Move to next message in batch
        }

        // 1. Update status to PROCESSING
        jobMetadata!.status = 'PROCESSING';
        jobMetadata!.startedAt = new Date().toISOString();
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        console.log(`Job ${jobId} started processing.`);

        // --- Core Backfill Logic ---

        // Step 1: Gemini API call
        jobMetadata!.progress = 'Step 1/6: Calling Gemini API...';
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        const geminiResult = await callGeminiAPI(payload.userId);
        console.log(geminiResult);

        // Step 2: 20 sequential ISBNdb API calls
        const isbnsToLookup = Array.from({ length: 20 }, (_, i) => `978-0321765723-${i}`); // Example ISBNs
        const isbnResults: string[] = [];
        for (let i = 0; i < isbnsToLookup.length; i++) {
          jobMetadata!.progress = `Step 2/6: Resolving ISBNs (${i + 1}/${isbnsToLookup.length} done)...`;
          await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
          const result = await callISBNdbAPI(isbnsToLookup[i], env);
          isbnResults.push(result);
        }

        // Step 3: Database deduplication queries
        jobMetadata!.progress = 'Step 3/6: Running database deduplication...';
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        await deduplicateDatabase(isbnResults); // Pass relevant data

        // Step 4: ISBNdb batch enrichment
        jobMetadata!.progress = 'Step 4/6: Performing ISBNdb batch enrichment...';
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        const enrichedData = await isbnDbBatchEnrichment(isbnsToLookup);

        // Step 5: Database writes
        jobMetadata!.progress = 'Step 5/6: Writing to database...';
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        await writeToDatabase(enrichedData); // Pass relevant data

        // Step 6: Finalization
        jobMetadata!.progress = 'Step 6/6: Finalizing job...';
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));

        // --- End Core Backfill Logic ---

        // Update status to COMPLETED
        jobMetadata!.status = 'COMPLETED';
        jobMetadata!.completedAt = new Date().toISOString();
        jobMetadata!.progress = 'Completed.';
        jobMetadata!.results = {
          totalRecordsProcessed: isbnsToLookup.length,
          newRecordsAdded: isbnsToLookup.length, // Placeholder
        };
        await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        console.log(`Job ${jobId} completed successfully.`);

        message.ack(); // Acknowledge the message to remove it from the queue
      } catch (error: any) {
        console.error(`Error processing job ${jobId}:`, error);
        if (jobMetadata) {
          jobMetadata.status = 'FAILED';
          jobMetadata.completedAt = new Date().toISOString();
          jobMetadata.progress = 'Failed.';
          jobMetadata.error = error.message || 'Unknown error';
          await env.JOB_STATUS_KV.put(jobId, JSON.stringify(jobMetadata));
        }

        // Nack the message to send it back to the queue for retries
        // Or consider using message.retry(delay) for specific delays
        // If this is a persistent error, consider a Dead-Letter Queue (DLQ) strategy.
        message.nack();
      }
    }
  },
};
</NEWFILE>

<NEWFILE: src/status_worker.ts>
import { Request, Router, IRequest } from 'itty-router';
import { Env } from './env'; // Assume an env.ts for environment types
import { BackfillJobMetadata } from './types';

// Define your router for the status endpoint
const statusRouter = Router({ base: '/api/harvest' });

/**
 * Handles the HTTP GET request to retrieve the status of a backfill job.
 * Fetches the job metadata from KV.
 *
 * @param request The incoming HTTP request.
 * @param env The environment variables and bindings.
 * @param ctx The Worker context.
 * @returns A JSON response with the job status.
 */
statusRouter.get('/backfill/status/:jobId', async (request: IRequest, env: Env, ctx: ExecutionContext) => {
  const jobId = request.params?.jobId;

  if (!jobId) {
    return new Response(JSON.stringify({ error: 'Missing job ID' }), {
      status: 400,
      headers: { 'Content-Type': 'application/json' },
    });
  }

  try {
    const jobStatusString = await env.JOB_STATUS_KV.get(jobId);

    if (!jobStatusString) {
      return new Response(JSON.stringify({ error: `Job with ID ${jobId} not found.` }), {
        status: 404,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const jobStatus: BackfillJobMetadata = JSON.parse(jobStatusString);
    return new Response(JSON.stringify(jobStatus), {
      status: 200,
      headers: { 'Content-Type': 'application/json' },
    });
  } catch (error) {
    console.error(`Error fetching status for job ${jobId}:`, error);
    return new Response(JSON.stringify({ error: 'Internal server error' }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
});

export default statusRouter;
</NEWFILE>

<UPDATED_EXISTING_FILE: src/index.ts>
// Assuming src/index.ts is your main entry point for HTTP requests.
// You would typically import and combine multiple routers here.

import { Router, IRequest } from 'itty-router';
import apiRouter from './api_worker';
import statusRouter from './status_worker';
import queueConsumer from './queue_consumer_worker'; // Import the queue consumer
import { Env } from './env';

// Base router for HTTP requests
const baseRouter = Router();

// Register the API and Status routers
baseRouter.all('/api/harvest/*', apiRouter.handle);
baseRouter.all('/api/harvest/backfill/status/:jobId', statusRouter.handle);

// Catch-all for unhandled routes
baseRouter.all('*', () => new Response('Not Found.', { status: 404 }));

export default {
  // HTTP requests are handled by the fetch handler
  async fetch(request: IRequest, env: Env, ctx: ExecutionContext): Promise<Response> {
    return baseRouter.handle(request, env, ctx);
  },

  // Queue messages are handled by the queue handler
  async queue(batch: MessageBatch<any>, env: Env, ctx: ExecutionContext): Promise<void> {
    await queueConsumer.queue(batch, env, ctx);
  }
};
</UPDATED_EXISTING_FILE>

<NEWFILE: src/env.ts>
// src/env.ts
// This file defines the shape of your environment variables and bindings.

interface Env {
  // KV Namespace binding
  JOB_STATUS_KV: KVNamespace;

  // Queue producer binding
  HARVEST_BACKFILL_QUEUE: Queue;

  // Your API keys from wrangler.toml vars
  ISBNDB_API_KEY: string;
  GEMINI_API_KEY: string;

  // Add other bindings (e.g., D1, R2, etc.) as needed
  // DB: D1Database;
  // R2_BUCKET: R2Bucket;
}

export { Env };
</NEWFILE>
</GENERATED-CODE>
