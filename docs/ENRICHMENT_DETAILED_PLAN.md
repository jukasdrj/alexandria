# Alexandria Enrichment Pipeline - Detailed Implementation Plan

**Generated by:** Grok (grok-4-1-fast-non-reasoning)  
**Date:** November 29, 2025  
**Status:** Ready for Implementation

---

## PHASE 1: Alexandria Write Endpoints (2-3 hours)

### 1. Files to Create/Modify
```
alexandria/worker/src/routes/enrich.ts          # New: Enrichment API routes
alexandria/worker/src/schemas/enrich.ts         # New: Zod validation schemas
alexandria/worker/src/services/enrichment.ts    # New: Business logic for upserts
alexandria/worker/src/types/enrichment.ts       # New: TypeScript interfaces
alexandria/wrangler.jsonc                       # Update: Add routes, auth binding
```

### 2. Specific Code Patterns/Examples

**Zod Schemas (schemas/enrich.ts):**
```typescript
import { z } from 'zod';

export const EnrichEditionSchema = z.object({
  isbn: z.string().min(10).max(13),
  work_key: z.string().optional(),
  title: z.string().optional(),
  publisher: z.string().optional(),
  page_count: z.number().int().min(1).optional(),
  format: z.enum(['hardcover', 'paperback', 'ebook', 'audiobook']).optional(),
  metadata: z.record(z.any()).optional()
});

export const EnrichWorkSchema = z.object({
  work_key: z.string(),
  title: z.string(),
  subtitle: z.string().optional(),
  description: z.string().optional(),
  original_language: z.string().optional(),
  first_publication_year: z.number().int().optional(),
  subject_tags: z.array(z.string()).optional(),
  cover_url_large: z.string().url().optional(),
  cover_url_medium: z.string().url().optional(),
  cover_url_small: z.string().url().optional(),
  metadata: z.record(z.any()).optional()
});

export const EnrichAuthorSchema = z.object({
  author_key: z.string(),
  name: z.string(),
  gender: z.enum(['Male', 'Female', 'NonBinary', 'Unknown']).optional(),
  nationality: z.string().optional(),
  birth_year: z.number().int().optional(),
  death_year: z.number().int().optional(),
  bio: z.string().optional(),
  author_photo_url: z.string().url().optional(),
  metadata: z.record(z.any()).optional()
});

export const EnrichQueueSchema = z.object({
  entity_type: z.enum(['work', 'edition', 'author']),
  entity_key: z.string(),
  priority: z.number().int().min(1).max(10).default(5),
  providers_to_try: z.array(z.enum(['isbndb', 'google-books', 'wikidata'])).default(['isbndb', 'google-books'])
});

export type EnrichEditionInput = z.infer<typeof EnrichEditionSchema>;
export type EnrichWorkInput = z.infer<typeof EnrichWorkSchema>;
export type EnrichAuthorInput = z.infer<typeof EnrichAuthorSchema>;
export type EnrichQueueInput = z.infer<typeof EnrichQueueSchema>;
```

**Route Handler (routes/enrich.ts):**
```typescript
import { Hono } from 'hono';
import { zValidator } from '@hono/zod-validator';
import { 
  EnrichEditionSchema, 
  EnrichWorkSchema, 
  EnrichAuthorSchema,
  EnrichQueueSchema 
} from '../schemas/enrich';
import { 
  upsertEnrichedEdition, 
  upsertEnrichedWork,
  upsertEnrichedAuthor,
  queueEnrichment,
  getQueueStatus
} from '../services/enrichment';

const app = new Hono();

// Auth middleware - check API key
app.use('*', async (c, next) => {
  const apiKey = c.req.header('X-API-Key');
  if (apiKey !== c.env.ALEXANDRIA_INTERNAL_API_KEY) {
    return c.json({ error: 'Unauthorized' }, 401);
  }
  await next();
});

app.post('/edition', zValidator('json', EnrichEditionSchema), async (c) => {
  const data = c.req.valid('json');
  const result = await upsertEnrichedEdition(c.env, data);
  return c.json(result, 201);
});

app.post('/work', zValidator('json', EnrichWorkSchema), async (c) => {
  const data = c.req.valid('json');
  const result = await upsertEnrichedWork(c.env, data);
  return c.json(result, 201);
});

app.post('/author', zValidator('json', EnrichAuthorSchema), async (c) => {
  const data = c.req.valid('json');
  const result = await upsertEnrichedAuthor(c.env, data);
  return c.json(result, 201);
});

app.post('/queue', zValidator('json', EnrichQueueSchema), async (c) => {
  const data = c.req.valid('json');
  const result = await queueEnrichment(c.env, data);
  return c.json({ id: result.id, status: 'pending' }, 201);
});

app.get('/status/:id', async (c) => {
  const id = c.req.param('id');
  const result = await getQueueStatus(c.env, id);
  if (!result) {
    return c.json({ error: 'Job not found' }, 404);
  }
  return c.json(result);
});

export default app;
```

### 3. SQL Queries Needed

**Upsert Enriched Edition:**
```sql
INSERT INTO enriched_editions (
  isbn, work_key, title, subtitle, publisher, publication_date, 
  page_count, format, language, cover_url_large, cover_url_medium, 
  cover_url_small, primary_provider, contributors, metadata, 
  created_at, updated_at
) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, ARRAY[$14], $15, NOW(), NOW())
ON CONFLICT (isbn) DO UPDATE SET
  work_key = COALESCE(EXCLUDED.work_key, enriched_editions.work_key),
  title = COALESCE(EXCLUDED.title, enriched_editions.title),
  publisher = COALESCE(EXCLUDED.publisher, enriched_editions.publisher),
  page_count = COALESCE(EXCLUDED.page_count, enriched_editions.page_count),
  format = COALESCE(EXCLUDED.format, enriched_editions.format),
  cover_url_large = COALESCE(EXCLUDED.cover_url_large, enriched_editions.cover_url_large),
  metadata = COALESCE(EXCLUDED.metadata, enriched_editions.metadata),
  contributors = array_cat(enriched_editions.contributors, ARRAY[$14]::text[]),
  updated_at = NOW()
RETURNING *;
```

**Upsert Enriched Work:**
```sql
INSERT INTO enriched_works (
  work_key, title, subtitle, description, original_language,
  first_publication_year, subject_tags, cover_url_large, 
  cover_url_medium, cover_url_small, primary_provider, 
  contributors, metadata, created_at, updated_at
) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, ARRAY[$12], $13, NOW(), NOW())
ON CONFLICT (work_key) DO UPDATE SET
  title = COALESCE(EXCLUDED.title, enriched_works.title),
  description = COALESCE(EXCLUDED.description, enriched_works.description),
  subject_tags = COALESCE(EXCLUDED.subject_tags, enriched_works.subject_tags),
  cover_url_large = COALESCE(EXCLUDED.cover_url_large, enriched_works.cover_url_large),
  metadata = COALESCE(EXCLUDED.metadata, enriched_works.metadata),
  contributors = array_cat(enriched_works.contributors, ARRAY[$12]::text[]),
  updated_at = NOW()
RETURNING *;
```

**Upsert Enriched Author:**
```sql
INSERT INTO enriched_authors (
  author_key, name, gender, nationality, birth_year, death_year,
  bio, author_photo_url, primary_provider, contributors, 
  metadata, created_at, updated_at
) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, ARRAY[$10], $11, NOW(), NOW())
ON CONFLICT (author_key) DO UPDATE SET
  name = COALESCE(EXCLUDED.name, enriched_authors.name),
  gender = COALESCE(EXCLUDED.gender, enriched_authors.gender),
  bio = COALESCE(EXCLUDED.bio, enriched_authors.bio),
  author_photo_url = COALESCE(EXCLUDED.author_photo_url, enriched_authors.author_photo_url),
  metadata = COALESCE(EXCLUDED.metadata, enriched_authors.metadata),
  contributors = array_cat(enriched_authors.contributors, ARRAY[$10]::text[]),
  updated_at = NOW()
RETURNING *;
```

**Insert Queue Item:**
```sql
INSERT INTO enrichment_queue (
  id, entity_type, entity_key, priority, status, 
  providers_to_try, created_at
) VALUES (gen_random_uuid(), $1, $2, $3, 'pending', $4, NOW())
RETURNING id, status, created_at;
```

**Log Enrichment Operation:**
```sql
INSERT INTO enrichment_log (
  id, entity_type, entity_key, provider, operation, 
  success, fields_updated, error_message, response_time_ms, created_at
) VALUES (gen_random_uuid(), $1, $2, $3, $4, $5, $6, $7, $8, NOW());
```

**Get Queue Status:**
```sql
SELECT id, entity_type, entity_key, priority, status, 
       providers_to_try, providers_attempted, providers_succeeded,
       created_at, started_at, completed_at, error_message, retry_count
FROM enrichment_queue 
WHERE id = $1;
```

### 4. Step-by-Step Implementation Order
1. Create `worker/src/schemas/enrich.ts` with Zod schemas for all 5 endpoints
2. Create `worker/src/types/enrichment.ts` with TypeScript interfaces matching schema shapes
3. Create `worker/src/services/enrichment.ts` with upsert functions for works/editions/authors
4. Create `worker/src/routes/enrich.ts` with Hono routes + Zod validation middleware
5. Add auth middleware (API key header check)
6. Register routes in main `worker/src/index.ts`
7. Add `ALEXANDRIA_INTERNAL_API_KEY` secret to wrangler
8. Deploy and test each endpoint individually

### 5. Testing Checklist
- [ ] POST /api/enrich/edition creates new record, returns 201
- [ ] POST /api/enrich/edition updates existing record idempotently  
- [ ] Invalid ISBN returns 400 with Zod error details
- [ ] Missing API key returns 401
- [ ] POST /api/enrich/work creates/updates work record
- [ ] POST /api/enrich/author creates/updates author record
- [ ] POST /api/enrich/queue inserts to enrichment_queue table
- [ ] GET /api/enrich/status/:id returns 404 for missing ID
- [ ] GET /api/enrich/status/:id returns correct status for valid ID
- [ ] enrichment_log captures all operations with correct fields_updated

---

## PHASE 2: bendv3 Cache Simplification (1-2 hours)

### 1. Files to Create/Modify
```
bendv3/src/services/alexandria-api.ts     # Major refactor: Remove KV cache
bendv3/src/services/external-apis.ts      # Add fire-and-forget POST to Alexandria
bendv3/wrangler.jsonc                     # Add ALEXANDRIA_API_KEY secret binding
```

### 2. Specific Code Patterns/Examples

**Simplified Alexandria Lookup (alexandria-api.ts):**
```typescript
/**
 * Alexandria API Integration - SIMPLIFIED
 * 
 * No more KV cache! PostgreSQL + Hyperdrive IS the cache.
 * Circuit breaker provides protection.
 */

import { withCircuitBreaker } from "./circuit-breaker";
import { normalizeAlexandriaResponse } from "./normalizers/alexandria";
import type { NormalizedResponse } from "./external-apis";

const ALEXANDRIA_BASE_URL = "https://alexandria.ooheynerds.com";

export async function searchAlexandriaByISBN(
  isbn: string,
  env: ExternalAPIEnv,
  ctx?: ExecutionContext
): Promise<NormalizedResponse | null> {
  // NO KV CACHE - Alexandria's PostgreSQL IS the cache!
  return withCircuitBreaker('alexandria', env, async () => {
    console.log(`üìö Alexandria lookup: ${isbn}`);
    
    const response = await fetch(
      `${ALEXANDRIA_BASE_URL}/api/isbn?isbn=${encodeURIComponent(isbn)}`,
      {
        headers: {
          'Accept': 'application/json',
          'User-Agent': 'BooksTracker/1.0'
        }
      }
    );

    if (response.status === 404) {
      console.log(`üì≠ Alexandria: ISBN ${isbn} not found`);
      return null;
    }

    if (!response.ok) {
      throw new Error(`Alexandria API error: ${response.status}`);
    }

    const data = await response.json();
    
    if (!data.results || data.results.length === 0) {
      return null;
    }

    return normalizeAlexandriaResponse(data.results[0], isbn);
  });
}
```

**Fire-and-Forget Enrichment (add to external-apis.ts):**
```typescript
/**
 * Post book data to Alexandria for permanent storage
 * Called when using Google Books fallback so future lookups are free
 */
async function enrichAlexandria(
  bookData: NormalizedResponse,
  isbn: string,
  env: ExternalAPIEnv,
  ctx: ExecutionContext
) {
  // Fire-and-forget using waitUntil
  ctx.waitUntil((async () => {
    try {
      const apiKey = env.ALEXANDRIA_API_KEY?.get 
        ? await env.ALEXANDRIA_API_KEY.get() 
        : env.ALEXANDRIA_API_KEY;
        
      if (!apiKey) {
        console.warn('ALEXANDRIA_API_KEY not configured, skipping enrichment');
        return;
      }

      const work = bookData.works[0];
      const edition = bookData.editions[0];

      // POST edition to Alexandria
      const response = await fetch(
        'https://alexandria.ooheynerds.com/api/enrich/edition',
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'X-API-Key': apiKey
          },
          body: JSON.stringify({
            isbn: isbn.replace(/-/g, ''),
            title: work?.title,
            publisher: edition?.publisher,
            page_count: edition?.pageCount,
            format: edition?.format,
            // Add more fields as needed
          })
        }
      );

      if (response.ok) {
        console.log(`‚úÖ Enriched Alexandria with ISBN ${isbn}`);
      } else {
        console.warn(`‚ö†Ô∏è Alexandria enrichment failed: ${response.status}`);
      }
    } catch (error) {
      console.error('Alexandria enrichment error:', error);
      // Don't throw - enrichment is best-effort
    }
  })());
}

// Update searchByISBN to call enrichAlexandria on fallback
export async function searchByISBN(
  isbn: string,
  env: ExternalAPIEnv,
  ctx?: ExecutionContext
): Promise<NormalizedResponse | null> {
  // 1. Try Alexandria first (no cache overhead!)
  const alexandriaResult = await searchAlexandriaByISBN(isbn, env, ctx);
  if (alexandriaResult && alexandriaResult.works.length > 0) {
    console.log(`‚úÖ ISBN ${isbn} from Alexandria`);
    return alexandriaResult;
  }

  // 2. Fallback to Google Books
  const googleResult = await searchGoogleBooksByISBN(isbn, env, ctx);
  if (googleResult && googleResult.works.length > 0) {
    console.log(`‚úÖ ISBN ${isbn} from Google Books (fallback)`);
    
    // 3. Enrich Alexandria so future lookups are free
    if (ctx) {
      enrichAlexandria(googleResult, isbn, env, ctx);
    }
    
    return googleResult;
  }

  return null;
}
```

### 3. Environment Updates

**Add to bendv3 wrangler.jsonc secrets_store_secrets:**
```jsonc
{
  "binding": "ALEXANDRIA_API_KEY",
  "store_id": "b0562ac16fde468c8af12717a6c88400",
  "secret_name": "ALEXANDRIA_INTERNAL_API_KEY"
}
```

### 4. Step-by-Step Implementation Order
1. Add `ALEXANDRIA_API_KEY` to Cloudflare Secrets Store
2. Update bendv3 `wrangler.jsonc` with secret binding
3. Refactor `alexandria-api.ts` - remove all KV cache logic
4. Add `enrichAlexandria()` function to `external-apis.ts`
5. Update `searchByISBN()` to call `enrichAlexandria()` on fallback
6. Deploy to staging, verify logs show Alexandria enrichment
7. Deploy to production

### 5. Testing Checklist
- [ ] ISBN lookup latency unchanged (<30ms p95 for Alexandria hits)
- [ ] No more KV writes for book metadata (check `wrangler tail` logs)
- [ ] Google Books fallback POSTs to Alexandria (check Alexandria logs)
- [ ] Circuit breaker still protects against Alexandria outages
- [ ] Popular ISBNs still return instantly
- [ ] New ISBNs from Google Books appear in Alexandria DB after lookup

---

## PHASE 3: Alexandria Enrichment Processor (3-4 hours)

### 1. Files to Create/Modify
```
alexandria/worker/src/cron/enrichment-processor.ts  # New: Main cron handler
alexandria/worker/src/services/isbndb-client.ts     # New: Rate-limited ISBNdb client
alexandria/worker/src/services/google-books.ts      # New: Google Books client
alexandria/worker/src/services/quality-scorer.ts    # New: Scoring algorithms
alexandria/wrangler.jsonc                           # Add cron trigger + secrets
```

### 2. Specific Code Patterns/Examples

**Rate-Limited ISBNdb Client (services/isbndb-client.ts):**
```typescript
/**
 * ISBNdb API Client with 1 req/sec rate limiting
 */

let lastRequestTime = 0;

async function enforceRateLimit() {
  const now = Date.now();
  const timeSinceLastRequest = now - lastRequestTime;
  
  if (timeSinceLastRequest < 1000) {
    await new Promise(resolve => 
      setTimeout(resolve, 1000 - timeSinceLastRequest)
    );
  }
  
  lastRequestTime = Date.now();
}

export interface ISBNdbBook {
  isbn: string;
  isbn13: string;
  title: string;
  title_long?: string;
  publisher?: string;
  pages?: number;
  binding?: string;
  language?: string;
  date_published?: string;
  authors?: string[];
  subjects?: string[];
  synopsis?: string;
  image?: string;
}

export async function lookupISBN(
  isbn: string, 
  apiKey: string
): Promise<ISBNdbBook | null> {
  await enforceRateLimit();
  
  const response = await fetch(
    `https://api2.isbndb.com/book/${isbn}`,
    {
      headers: {
        'Authorization': apiKey,
        'Accept': 'application/json'
      }
    }
  );

  if (!response.ok) {
    if (response.status === 404) return null;
    throw new Error(`ISBNdb error: ${response.status}`);
  }

  const data = await response.json();
  return data.book || null;
}

export async function searchByTitle(
  title: string,
  author: string | null,
  apiKey: string
): Promise<ISBNdbBook[]> {
  await enforceRateLimit();
  
  let url = `https://api2.isbndb.com/books/${encodeURIComponent(title)}?page=1&pageSize=10`;
  if (author) {
    url += `&author=${encodeURIComponent(author)}`;
  }

  const response = await fetch(url, {
    headers: {
      'Authorization': apiKey,
      'Accept': 'application/json'
    }
  });

  if (!response.ok) {
    throw new Error(`ISBNdb search error: ${response.status}`);
  }

  const data = await response.json();
  return data.books || [];
}
```

**Quality Scorer (services/quality-scorer.ts):**
```typescript
/**
 * Calculate quality and completeness scores for enriched data
 */

interface ScoringFields {
  title: boolean;
  description: boolean;
  publisher: boolean;
  page_count: boolean;
  publication_date: boolean;
  subjects: boolean;
  cover_url: boolean;
  language: boolean;
}

export function calculateCompletenessScore(data: any): number {
  const fields: ScoringFields = {
    title: !!data.title && data.title.length > 0,
    description: !!data.description && data.description.length > 50,
    publisher: !!data.publisher,
    page_count: !!data.page_count && data.page_count > 0,
    publication_date: !!data.publication_date,
    subjects: Array.isArray(data.subjects) && data.subjects.length > 0,
    cover_url: !!data.cover_url_large || !!data.cover_url_medium,
    language: !!data.language
  };

  const filledFields = Object.values(fields).filter(Boolean).length;
  const totalFields = Object.keys(fields).length;
  
  return Math.round((filledFields / totalFields) * 100);
}

export function calculateISBNdbQuality(book: any): number {
  let score = 0;
  
  // Base score for having data
  if (book.title) score += 15;
  if (book.publisher) score += 10;
  if (book.pages && book.pages > 0) score += 10;
  if (book.date_published) score += 10;
  if (book.synopsis && book.synopsis.length > 100) score += 15;
  if (book.subjects && book.subjects.length > 0) score += 10;
  if (book.image) score += 10;
  if (book.authors && book.authors.length > 0) score += 10;
  if (book.language) score += 5;
  if (book.binding) score += 5;
  
  return Math.min(score, 100);
}
```

**Enrichment Processor (cron/enrichment-processor.ts):**
```typescript
/**
 * Background enrichment processor
 * Runs every 15 minutes via Cloudflare Cron
 */

import { lookupISBN } from '../services/isbndb-client';
import { calculateCompletenessScore, calculateISBNdbQuality } from '../services/quality-scorer';

const BATCH_SIZE = 10;

export async function processEnrichmentQueue(env: Env, ctx: ExecutionContext) {
  console.log('üîÑ Starting enrichment queue processing...');
  
  // Fetch pending jobs with row-level locking
  const jobs = await env.DB.prepare(`
    SELECT * FROM enrichment_queue 
    WHERE status = 'pending' 
    ORDER BY priority DESC, created_at ASC 
    LIMIT $1
  `).bind(BATCH_SIZE).all();

  if (!jobs.results || jobs.results.length === 0) {
    console.log('üì≠ No pending enrichment jobs');
    return;
  }

  console.log(`üìã Processing ${jobs.results.length} enrichment jobs`);

  for (const job of jobs.results) {
    await processJob(env, job);
  }

  console.log('‚úÖ Enrichment queue processing complete');
}

async function processJob(env: Env, job: any) {
  const startTime = Date.now();
  
  try {
    // Mark job as processing
    await env.DB.prepare(`
      UPDATE enrichment_queue 
      SET status = 'processing', started_at = NOW() 
      WHERE id = $1
    `).bind(job.id).run();

    let success = false;
    let fieldsUpdated: string[] = [];
    let lastError: string | null = null;

    // Try each provider in order
    for (const provider of job.providers_to_try) {
      if (job.providers_attempted?.includes(provider)) continue;

      try {
        const result = await tryProvider(env, job, provider);
        
        if (result.success) {
          success = true;
          fieldsUpdated = [...fieldsUpdated, ...result.fieldsUpdated];
          
          // Log success
          await logEnrichment(env, job, provider, true, result.fieldsUpdated, null, Date.now() - startTime);
          
          // Update providers_succeeded
          await env.DB.prepare(`
            UPDATE enrichment_queue 
            SET providers_succeeded = array_cat(COALESCE(providers_succeeded, ARRAY[]::text[]), ARRAY[$1]::text[])
            WHERE id = $2
          `).bind(provider, job.id).run();
        }
      } catch (error) {
        lastError = error.message;
        await logEnrichment(env, job, provider, false, [], error.message, Date.now() - startTime);
      }

      // Update providers_attempted
      await env.DB.prepare(`
        UPDATE enrichment_queue 
        SET providers_attempted = array_cat(COALESCE(providers_attempted, ARRAY[]::text[]), ARRAY[$1]::text[])
        WHERE id = $2
      `).bind(provider, job.id).run();
    }

    // Mark job complete or failed
    const finalStatus = success ? 'completed' : 'failed';
    await env.DB.prepare(`
      UPDATE enrichment_queue 
      SET status = $1, completed_at = NOW(), error_message = $2
      WHERE id = $3
    `).bind(finalStatus, success ? null : lastError, job.id).run();

    console.log(`${success ? '‚úÖ' : '‚ùå'} Job ${job.id}: ${finalStatus}`);
    
  } catch (error) {
    console.error(`üí• Job ${job.id} failed:`, error);
    
    // Increment retry count
    await env.DB.prepare(`
      UPDATE enrichment_queue 
      SET status = 'pending', retry_count = retry_count + 1, error_message = $1
      WHERE id = $2 AND retry_count < max_retries
    `).bind(error.message, job.id).run();
    
    // Mark as failed if max retries exceeded
    await env.DB.prepare(`
      UPDATE enrichment_queue 
      SET status = 'failed', error_message = $1
      WHERE id = $2 AND retry_count >= max_retries
    `).bind(error.message, job.id).run();
  }
}

async function tryProvider(env: Env, job: any, provider: string): Promise<{ success: boolean, fieldsUpdated: string[] }> {
  switch (provider) {
    case 'isbndb':
      return await enrichFromISBNdb(env, job);
    case 'google-books':
      return await enrichFromGoogleBooks(env, job);
    case 'wikidata':
      return await enrichFromWikidata(env, job);
    default:
      throw new Error(`Unknown provider: ${provider}`);
  }
}

async function enrichFromISBNdb(env: Env, job: any): Promise<{ success: boolean, fieldsUpdated: string[] }> {
  if (job.entity_type !== 'edition') {
    return { success: false, fieldsUpdated: [] };
  }

  const apiKey = await env.ISBNDB_API_KEY.get();
  const book = await lookupISBN(job.entity_key, apiKey);
  
  if (!book) {
    return { success: false, fieldsUpdated: [] };
  }

  const quality = calculateISBNdbQuality(book);
  const completeness = calculateCompletenessScore({
    title: book.title,
    description: book.synopsis,
    publisher: book.publisher,
    page_count: book.pages,
    publication_date: book.date_published,
    subjects: book.subjects,
    cover_url_large: book.image,
    language: book.language
  });

  // Upsert to enriched_editions
  await env.DB.prepare(`
    INSERT INTO enriched_editions (
      isbn, title, publisher, page_count, format, publication_date,
      cover_url_large, primary_provider, contributors, 
      isbndb_quality, completeness_score, last_isbndb_sync,
      created_at, updated_at
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, 'isbndb', ARRAY['isbndb'], $8, $9, NOW(), NOW(), NOW())
    ON CONFLICT (isbn) DO UPDATE SET
      title = COALESCE(EXCLUDED.title, enriched_editions.title),
      publisher = COALESCE(EXCLUDED.publisher, enriched_editions.publisher),
      page_count = COALESCE(EXCLUDED.page_count, enriched_editions.page_count),
      format = COALESCE(EXCLUDED.format, enriched_editions.format),
      cover_url_large = COALESCE(EXCLUDED.cover_url_large, enriched_editions.cover_url_large),
      contributors = array_cat(enriched_editions.contributors, ARRAY['isbndb']::text[]),
      isbndb_quality = EXCLUDED.isbndb_quality,
      completeness_score = EXCLUDED.completeness_score,
      last_isbndb_sync = NOW(),
      updated_at = NOW()
  `).bind(
    job.entity_key,
    book.title,
    book.publisher,
    book.pages,
    book.binding,
    book.date_published,
    book.image,
    quality,
    completeness
  ).run();

  const fieldsUpdated = [];
  if (book.title) fieldsUpdated.push('title');
  if (book.publisher) fieldsUpdated.push('publisher');
  if (book.pages) fieldsUpdated.push('page_count');
  if (book.image) fieldsUpdated.push('cover_url_large');

  return { success: true, fieldsUpdated };
}

async function enrichFromGoogleBooks(env: Env, job: any): Promise<{ success: boolean, fieldsUpdated: string[] }> {
  // Implementation similar to ISBNdb but using Google Books API
  // TODO: Implement
  return { success: false, fieldsUpdated: [] };
}

async function enrichFromWikidata(env: Env, job: any): Promise<{ success: boolean, fieldsUpdated: string[] }> {
  // For author enrichment - fetch from Wikidata
  // TODO: Implement
  return { success: false, fieldsUpdated: [] };
}

async function logEnrichment(
  env: Env, 
  job: any, 
  provider: string, 
  success: boolean, 
  fieldsUpdated: string[],
  errorMessage: string | null,
  responseTimeMs: number
) {
  await env.DB.prepare(`
    INSERT INTO enrichment_log (
      id, entity_type, entity_key, provider, operation, 
      success, fields_updated, error_message, response_time_ms, created_at
    ) VALUES (gen_random_uuid(), $1, $2, $3, 'enrich', $4, $5, $6, $7, NOW())
  `).bind(
    job.entity_type,
    job.entity_key,
    provider,
    success,
    fieldsUpdated,
    errorMessage,
    responseTimeMs
  ).run();
}
```

### 3. Wrangler Configuration Updates

**Add to Alexandria wrangler.jsonc:**
```jsonc
{
  "triggers": {
    "crons": ["*/15 * * * *"]
  },
  "secrets_store_secrets": [
    {
      "binding": "ISBNDB_API_KEY",
      "store_id": "b0562ac16fde468c8af12717a6c88400",
      "secret_name": "ISBNDB_API_KEY"
    },
    {
      "binding": "GOOGLE_BOOKS_API_KEY", 
      "store_id": "b0562ac16fde468c8af12717a6c88400",
      "secret_name": "Google_books_hardoooe"
    }
  ]
}
```

### 4. Step-by-Step Implementation Order
1. Create ISBNdb client with rate limiting
2. Create Google Books client
3. Implement quality scoring functions
4. Create job processor with provider chain
5. Create cron handler with batch processing
6. Add cron trigger to wrangler.jsonc
7. Add secret bindings
8. Deploy and queue test jobs
9. Monitor first few cron runs

### 5. Testing Checklist
- [ ] Cron runs every 15 min (check Cloudflare dashboard)
- [ ] Processes up to 10 jobs per run
- [ ] ISBNdb rate limiting respected (1 req/sec)
- [ ] Failed providers logged, job retried up to max_retries
- [ ] Successful enrichment updates quality scores
- [ ] enrichment_log populated with all attempts
- [ ] Jobs marked 'completed' or 'failed' appropriately

---

## PHASE 4: Warming Strategy Audit (2 hours)

### 1. Current State Analysis

**bendv3 Cron Jobs (from wrangler.jsonc):**
```
"0 2 * * *"     ‚Üí Daily 2am archival
"*/15 * * * *"  ‚Üí Every 15min alerts
"0 3 * * *"     ‚Üí Daily 3am cover harvest
"0 0 * * SUN"   ‚Üí Sunday midnight weekly recommendations
"0 */6 * * *"   ‚Üí Every 6 hours static popular books cache warm-up
"0 * * * *"     ‚Üí Hourly analytics-driven cache warm-up
```

### 2. Decision Matrix

| Cron | Purpose | Keep/Remove | Reason |
|------|---------|-------------|--------|
| `0 2 * * *` | Archival | **KEEP** | User data archival still needed |
| `*/15 * * * *` | Alerts | **KEEP** | System health monitoring |
| `0 3 * * *` | Cover harvest | **EVALUATE** | May move to Alexandria |
| `0 0 * * SUN` | Weekly recommendations | **KEEP** | User-facing feature |
| `0 */6 * * *` | Popular book warming | **REMOVE** | Alexandria has 54M books |
| `0 * * * *` | Analytics warming | **REMOVE** | Redundant with Alexandria |

### 3. Recommended Actions

**Remove from bendv3:**
```typescript
// DELETE these warming functions:
// - warmPopularBooks()
// - warmAnalyticsDrivenCache()
// - any book metadata pre-caching

// Keep:
// - User data warming
// - Recommendation generation
// - Cover harvesting (evaluate moving to Alexandria)
```

**Update bendv3 wrangler.jsonc:**
```jsonc
{
  "triggers": {
    "crons": [
      "0 2 * * *",      // Daily archival (KEEP)
      "*/15 * * * *",   // Alerts (KEEP)
      "0 3 * * *",      // Cover harvest (EVALUATE)
      "0 0 * * SUN"     // Weekly recommendations (KEEP)
      // REMOVED: "0 */6 * * *" (popular book warming)
      // REMOVED: "0 * * * *" (analytics warming)
    ]
  }
}
```

### 4. Testing Checklist
- [ ] No cron errors after removing book warming
- [ ] ISBN lookup latency unchanged (<30ms)
- [ ] User data features unaffected
- [ ] Weekly recommendations still generate
- [ ] Alerts still fire correctly

---

## PHASE 5: OpenLibrary Update Strategy (4-6 hours)

### 1. Recommended Strategy: Hybrid Approach

```
Monthly (1st):   Full dump refresh of base tables (editions, works, authors)
Daily (3am):     Incremental sync via OpenLibrary Recent Changes API
Real-time:       enrichment_queue handles user-triggered updates
```

### 2. Database Migration

**File: migrations/002_sync_metadata.sql**
```sql
-- Sync metadata tracking table
CREATE TABLE IF NOT EXISTS sync_metadata (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Initialize sync tracking
INSERT INTO sync_metadata (key, value) VALUES 
  ('ol_last_changes_sync', '2025-11-29T00:00:00Z'),
  ('ol_full_dump_version', '2025-11'),
  ('ol_full_dump_last_run', '2025-11-29T00:00:00Z')
ON CONFLICT (key) DO NOTHING;

-- Index for faster lookups
CREATE INDEX IF NOT EXISTS idx_sync_metadata_key ON sync_metadata(key);
```

### 3. OpenLibrary Recent Changes API Client

**File: services/openlibrary-sync.ts**
```typescript
/**
 * OpenLibrary Recent Changes API Client
 * https://openlibrary.org/dev/docs/api/recentchanges
 */

const OL_BASE_URL = 'https://openlibrary.org';

interface RecentChange {
  id: string;
  kind: string;
  timestamp: string;
  author: { key: string };
  changes: Array<{ key: string; revision: number }>;
}

export async function fetchRecentChanges(
  since: string, 
  limit: number = 100
): Promise<RecentChange[]> {
  const url = `${OL_BASE_URL}/recentchanges.json?limit=${limit}&since=${encodeURIComponent(since)}`;
  
  const response = await fetch(url, {
    headers: { 'User-Agent': 'Alexandria/1.0' }
  });
  
  if (!response.ok) {
    throw new Error(`OpenLibrary API error: ${response.status}`);
  }
  
  return response.json();
}

export async function processRecentChanges(env: Env) {
  // Get last sync time
  const result = await env.DB.prepare(
    `SELECT value FROM sync_metadata WHERE key = 'ol_last_changes_sync'`
  ).first();
  
  const since = result?.value || '2025-11-29T00:00:00Z';
  
  console.log(`üîÑ Fetching OpenLibrary changes since ${since}`);
  
  const changes = await fetchRecentChanges(since);
  
  if (changes.length === 0) {
    console.log('üì≠ No new changes');
    return;
  }
  
  console.log(`üìã Processing ${changes.length} changes`);
  
  for (const change of changes) {
    for (const item of change.changes) {
      if (item.key.startsWith('/works/')) {
        // Queue work for re-fetching
        await queueWorkUpdate(env, item.key);
      } else if (item.key.startsWith('/books/')) {
        // Queue edition for re-fetching
        await queueEditionUpdate(env, item.key);
      }
    }
  }
  
  // Update last sync time
  const lastTimestamp = changes[changes.length - 1].timestamp;
  await env.DB.prepare(
    `UPDATE sync_metadata SET value = $1, updated_at = NOW() WHERE key = 'ol_last_changes_sync'`
  ).bind(lastTimestamp).run();
  
  console.log(`‚úÖ Synced up to ${lastTimestamp}`);
}

async function queueWorkUpdate(env: Env, workKey: string) {
  await env.DB.prepare(`
    INSERT INTO enrichment_queue (id, entity_type, entity_key, priority, status, providers_to_try, created_at)
    VALUES (gen_random_uuid(), 'work', $1, 3, 'pending', ARRAY['openlibrary'], NOW())
    ON CONFLICT DO NOTHING
  `).bind(workKey).run();
}

async function queueEditionUpdate(env: Env, editionKey: string) {
  await env.DB.prepare(`
    INSERT INTO enrichment_queue (id, entity_type, entity_key, priority, status, providers_to_try, created_at)
    VALUES (gen_random_uuid(), 'edition', $1, 3, 'pending', ARRAY['openlibrary'], NOW())
    ON CONFLICT DO NOTHING
  `).bind(editionKey).run();
}
```

### 4. Monthly Full Dump Script

**File: scripts/monthly-dump-refresh.sh**
```bash
#!/bin/bash
# Monthly OpenLibrary dump refresh
# Run on Unraid server (Tower)

set -e

DUMP_DIR="/mnt/user/domains/OL_DB/dumps"
DB_CONTAINER="postgres"
DB_NAME="openlibrary"
DB_USER="openlibrary"

echo "üì• Downloading latest OpenLibrary dump..."
cd $DUMP_DIR

# Download latest dumps (editions, works, authors)
wget -N https://openlibrary.org/data/ol_dump_editions_latest.txt.gz
wget -N https://openlibrary.org/data/ol_dump_works_latest.txt.gz  
wget -N https://openlibrary.org/data/ol_dump_authors_latest.txt.gz

echo "üóÑÔ∏è Refreshing base tables..."

# Refresh editions table (preserves enriched_* tables!)
gunzip -c ol_dump_editions_latest.txt.gz | docker exec -i $DB_CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  TRUNCATE editions RESTART IDENTITY;
  COPY editions FROM STDIN;
"

# Similar for works and authors...

echo "üìù Updating sync metadata..."
docker exec $DB_CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  UPDATE sync_metadata 
  SET value = '$(date -u +%Y-%m-%dT%H:%M:%SZ)', updated_at = NOW()
  WHERE key = 'ol_full_dump_last_run';
"

echo "‚úÖ Monthly refresh complete!"
```

### 5. Testing Checklist
- [ ] sync_metadata table created with initial values
- [ ] Daily cron fetches changes since last sync
- [ ] New works/editions queued for enrichment
- [ ] Monthly script runs without errors
- [ ] Enriched data preserved during full dumps
- [ ] Sync lag < 24 hours for new releases

---

## Implementation Timeline

```
Week 1:
‚îú‚îÄ‚îÄ Day 1-2: Phase 1 (Write endpoints)
‚îú‚îÄ‚îÄ Day 3: Phase 2 (bendv3 simplification)
‚îî‚îÄ‚îÄ Day 4-5: Testing & fixes

Week 2:
‚îú‚îÄ‚îÄ Day 1-3: Phase 3 (Enrichment processor)
‚îú‚îÄ‚îÄ Day 4: Phase 4 (Warming audit)
‚îî‚îÄ‚îÄ Day 5: Testing & monitoring

Week 3:
‚îú‚îÄ‚îÄ Day 1-3: Phase 5 (OL sync strategy)
‚îú‚îÄ‚îÄ Day 4-5: End-to-end testing
‚îî‚îÄ‚îÄ Production rollout
```

---

## Open Questions for Justin

1. **API Key Authentication:** Use Cloudflare Access service token or custom API key header?
2. **ISBNdb Budget:** How many enrichments per month? (affects queue priority)
3. **Cover Harvesting:** Keep in bendv3 or move to Alexandria?
4. **Monitoring:** Set up alerts for enrichment failures?

---

**Last Updated:** November 29, 2025  
**Generated By:** Grok (grok-4-1-fast-non-reasoning)
